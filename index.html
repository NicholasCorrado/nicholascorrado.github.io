<!DOCTYPE html>
<head xmlns="http://www.w3.org/1999/html">
    <title>Nicholas E. Corrado</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Nicholas E. Corrado">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Nicholas E. Corrado">
	<meta property="og:description" content="PhD student, University of Wisconsin-Madison">
<!--    <meta property="og:image" content="https://nicholascorrado.github.io/images/me.jpg">-->
    <meta property="og:image" content="images/me_neurips2024.jpeg">
	<meta property="og:url" content="https://nicholascorrado.github.io/">
<!--	<meta name="twitter:card" content="summary_large_image">-->
<!--    <link rel="apple-touch-icon" href="files/ucsd-logo.png">-->
<!--    <link rel="icon" type="image/png" href="files/ucsd-logo.png">-->
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
<!--    <link href="https://fonts.cdnfonts.com/css/latin-modern-roman" rel="stylesheet">-->
<!--    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>-->
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R8SE61FCCV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-R8SE61FCCV');
</script>
<!--<div class="header noselect">-->
<!--    <div class="row">-->
<!--        <div class="header-profile-picture"></div>-->
<!--    </div>-->
<!--</div>-->
<div class="header noselect">
    <div class="header-div content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Nicholas E. Corrado</br>(at ACL 2025)</h1>
            </div>
            <div class="header-subtitle">
                Ph.D. student, University of Wisconsin-Madison
            </div>
            <div class="header-links">
                <a class="btn" href="mailto:ncorrado@wisc.edu">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?hl=en&user=7aEHl08AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> /
                <a class="btn" href="https://github.com/nicholascorrado">GitHub</a> /
<!--                <a class="btn" href="">Twitter</a> /-->
                <a class="btn" href="https://www.linkedin.com/in/nicholas-corrado-1b823a108">LinkedIn</a> /
                <a class="btn" href="files/cv.pdf">CV</a>
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
  I am a final-year Ph.D. student in Computer Sciences at the <a href="https://www.wisc.edu">University of Wisconsin-Madison</a>, advised by <a href="https://pages.cs.wisc.edu/~jphanna/">Josiah Hanna</a>.
  My research focuses on improving the data efficiency of reinforcement learning (RL) algorithms by designing better data collection strategies.
  Since RL algorithms typically require an impractical amount of interaction with a task to perform well, my work asks: <em>What data should we collect to learn as efficiently as possible?</em> and <em>How do we efficiently collect such data?</em>
  Towards this end, I currently work on:
        </p>
  <ol>
    <li>Adaptive sampling algorithms for on-policy policy gradient methods</li>
    <li>Synthetic data generation for off-policy and offline RL</li>
  </ol>
</p>
        <p>
            Previously, I was a research intern with <a href="https://spectrum.ieee.org/amazon-rufus"> Amazon's Rufus team </a> (working on multi-objective alignment for LLMs) and <a href="https://www.sandia.gov">Sandia National Laboratories</a> (working on RL for power grid management).
            During the first year of my PhD, I worked in databases with <a href="https://jigneshpatel.org">Jignesh Patel</a>.
            I received a BPhil in physics and a B.S. in mathematics from <a href="https://www.pitt.edu">University of Pittsburgh</a> where I studied high-energy physics with <a href="https://www.physicsandastronomy.pitt.edu/people/vladimir-savinov">Vladimir Savinov</a>.
        </p>

        <div style="display: flex; justify-content: center; margin: 20px 0;">
            <p style="
                border: 1px solid #ccc;
                border-radius: 12px;
                padding: 10px 20px;
                background-color: #f9f9f9;
                text-align: center;
                margin: 0;
            ">
                <span class="bold">I am looking for postdoc or research scientist opportunities starting Summer/Fall 2026.</span>
            </p>
        </div>
        <p>
            Feel free to drop an email if you're interested in chatting!
        </p>

    </div>
    <div>
        <h2 class="noselect">News</h2>
        <ul class="news-list">
            <li><span class="news-date">[July 2025]</span> &#127942; <a class="news-highlight" href="https://arxiv.org/abs/2506.17124">"When Can Model-Free Reinforcement Learning be Enough for Thinking?"</a> was awarded <em>Most-Thought-Provoking Paper</em> at Finding the Frame Workshop @ RLC 2025 (Oral)!</li>
            <li><span class="news-date">[July 2025]</span> <a class="news-highlight" href="https://arxiv.org/abs/2506.17124">"When Can Model-Free Reinforcement Learning be Enough for Thinking?"</a> and <a class="news-highlight" href="https://arxiv.org/abs/2311.08290">"On-Policy Policy Gradient Learning Without On-Policy Sampling"</a> accepted at Finding the Frame Workshop @ RLC 2025!</li>
            <li><span class="news-date">[May 2025]</span> <a class="news-highlight" href="https://arxiv.org/abs/2506.00569">"AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs"</a> accepted at ACL 2025 Main Conference!</li>
            <li><span class="news-date">[Nov 2024]</span> &#127942; I received the <a class="news-highlight" href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Review Award at NeurIPS 2024!</a></li>
            <li><span class="news-date">[July 2024]</span> I joined <a class="news-highlight" href="https://spectrum.ieee.org/amazon-rufus">Amazon's Rufus Team</a> as a research intern working with <a class="news-highlight" href="https://jkatzsam.github.io">Julian Katz-Samuels</a>!</li>
            <li><span class="news-date">[May 2024]</span> 1 paper accepted at RLC 2024!</li>
            <li><span class="news-date">[January 2024]</span> 1 paper accepted at ICLR 2024!</li>
            <li><span class="news-date">[October 2023]</span> I gave <a class="news-highlight" href="https://youtu.be/c34txISfpQo?si=yXuSqrf38CgFx-pW">a talk on adaptive off-policy sampling for on-policy learning</a> at the University of Edinburgh RL reading group!</li>
            <li><span class="news-date">[January 2023]</span> &#127942; I received the Sandia Employee Recognition Award!</li>
        </ul>
    </div>
    <div>
        <h2 class="noselect">Publications and Preprints</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/thinking.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2506.17124">When Can Model-Free Reinforcement Learning be Enough for Thinking?</a><br/>
                Josiah P. Hanna, <span class="bold">Nicholas E. Corrado</span><br/>
                <span class="italic"> Under Review</span><br/>
                &#127942; <span class="bold" style="color: #f09228;">Most-Thought-Provoking Paper</span> in <span class="italic"><a href="https://sites.google.com/view/findingtheframe/home">Finding the Frame Workshop @ RLC</a>, 2025</span> <b>(Oral)</b><br/>
                <a class="btn" href="https://arxiv.org/abs/2506.17124">arXiv</a> / <a class="btn" href="bibtex/hanna2025thinking.txt">bibtex</a>
                <p><span class="bold">TLDR:</span>
                    This paper answers the question: Under what conditions will model-free reinforcement learning give rise to thinking as a strategy for reward maximization?
                </p>
                <p class="thanks">
Another special thanks to Julian Katz-Samuels. Much of what I know about LLMs comes from working with him, and that foundation made it easy to contribute to this project.
                </p>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/marl.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Centralized Adaptive Sampling for Reliable Co-training of Independent Multi-Agent Policies</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Josiah P. Hanna<br/>
                <span class="italic">Under Review</span><br/>
<!--                <a class="btn" href="files/maprops.pdf">pdf (arXiv and bibtex coming soon!)</a>-->
                <a class="btn" href="files/maprops.pdf"> pdf (arXiv and bibtex coming soon!)</a>
<!--                / <a class="btn" href="bibtex/corrado2025maprops.txt">bibtex</a>-->
                <p>
                    <span class="bold">TLDR:</span>
<!--                    Independent policy gradient algorithms it is well-understood that they may not converge to the most preferred equilibrium even in no-conflict games.-->
<!--                    Independent on-policy policy gradient MARL algorithms are known to converge suboptimally when each agent’s policy gradient points toward a suboptimal equilibrium.-->
<!--                    In this work, we identify and characterize a subtler, lesser-known failure mode that arises <em>even when the expected policy gradient of each agent aligns with optimal behavior</em>.-->
<!--                    After collecting a finite set of trajectories, stochasticity in independent action sampling can cause the joint data distribution to deviate from the expected joint on-policy distribution.-->
<!--                This \textit{sampling error} w.r.t.\@ the joint on-policy distribution  produces inaccurate gradient estimates that can lead agents to-->
<!--                    stochasticity in action sampling can nevertheless cause agents to converge to suboptimal solutions.-->
<!--                    We show that-->
<!--                    stochasticity in action sampling can nevertheless cause agents to converge to suboptimal solutions.-->

                    We identify a subtle failure mode of independent on-policy MARL:
                    on-policy sampling can produce data that deviates from the expected joint on-policy distribution, yielding inaccurate gradients that can make agents converge suboptimally&mdash;<em>even when the expected gradient of each agent aligns with optimal behavior</em>.
                    We introduce adaptive sampling algorithm that reduces this <em>sampling error</em> w.r.t. the joint on-policy distribution, enabling agents to more reliability converge to an optimal equilibria.
                </p>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/ama.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2506.00569">AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Julian Katz-Samuels, Adithya Devraj, Hyokun Yun, Chao Zhang, Yi Xu, Yi Pan, Bing Yin, Trishul Chilimbi<br/>
                <span class="italic"> Association for Computational Linguistics (<a href="https://2025.aclweb.org">ACL</a>, Main Conference), 2025 </span><br/>
                <a class="btn" href="https://arxiv.org/abs/2506.00569">arXiv</a> / <a class="btn" href="bibtex/corrado2025automixalign.txt">bibtex</a>
                <p><span class="bold">TLDR:</span>
                Naively aligning LLMs across many datasets each targeting different tasks often yields a model that performs well on some tasks but not others.
                We introduce AutoMixAlign, a theoretically-grounded data mixing algorithm that adaptively mixes datasets during training to balance performance across tasks.
                </p>
<!--                How should-->
<!--                However,-->
<!--                We introduce AutoMixAlign, an theoretically-grounded data mixing algorithm that adaptively mixes datasets during training to balance-->
<!--performance across tasks.-->
                <p class="thanks">
                        Special thanks to Julian Katz-Samuels, my mentor at Amazon.
<!--                        , whose mentorship was instrumental to the success of this work.-->
                        I came into this project with no prior LLM experience, and Julian's guidance, support, and confidence in me made all the difference.
                </p>
<!--                <details>-->
<!--                    <summary>Abstract</summary>-->
<!--                    <p>-->
<!--                        When aligning large language models (LLMs), their performance on various tasks (such as being helpful, harmless, and honest) depends heavily on the composition of their training data. However, selecting a data mixture that achieves strong performance across all tasks is challenging. Existing approaches rely on large ablation studies, heuristics, or human intuition, but these can be prohibitively expensive and suboptimal. We study this problem in the setting of preference optimization via DPO and introduce AutoMixAlign (AMA), a theoretically-grounded algorithm that adaptively mixes datasets during training to balance performance across tasks. AMA first trains \textit{specialist models} for each task to determine losses that correspond to strong task performance. Then, it trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses deviate most from specialist model losses. To optimize this problem, we propose two algorithms: (1) AMA-R, which adaptively reweights the objective to prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence result follows from Sagawa et al. (2019), and we provide a convergence proof for AMA-S using online learning techniques such as EXP3. We evaluate AMA on several multitask alignment setups and find that AMA outperforms the standard alignment approach &#45;&#45; which simply optimizes the total loss across all tasks &#45;&#45; and also outperforms model merging methods.-->
<!--                    </p>-->
<!--                </details>-->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/props.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2311.08290">On-Policy Policy Gradient Learning Without On-Policy Sampling</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Josiah P. Hanna<br/>
                <span class="italic">Under Review</span><br/>
                Also in <span class="italic"><a href="https://sites.google.com/view/findingtheframe/home">Finding the Frame Workshop @ Reinforcement Learning Conference (RLC)</a></span>, 2025<br/>
                <a class="btn" href="https://arxiv.org/abs/2311.08290">arXiv</a> / <a class="btn" href="bibtex/corrado2023props.txt">bibtex</a>
                <p>
                    <span class="bold">TLDR:</span> On-policy learning requires on-policy data, <span class="italic">not on-policy sampling</span>!
                    We introduce an adaptive, off-policy sampling algorithm that produces on-policy data more efficiently than on-policy sampling, improving data efficiency.
                </p>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/guda.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2310.18247">Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Yuxiao Qu, John U. Balis, Adam Labiosa, & Josiah P. Hanna<br/>
                <span class="italic">Reinforcement Learning Conference (<a href="https://rl-conference.cc/2024/">RLC</a>)</span>, 2024<br/>
<!--                <a class="btn" href=projects/GuDA/>project page</a> / -->
                <a class="btn" href="https://arxiv.org/abs/2310.18247">arXiv</a> / <a class="btn" href="bibtex/corrado2024guda.txt">bibtex</a>
                <p>
                    <span class="bold">TLDR:</span> While offline RL algorithm can in principle learn from highly suboptimal data, they nevertheless perform <span class="italic">much</span> better with near expert-quality data.
                    Taking a "data first" perspective, we introduce a data augmentation framework that automatically generates near expert-quality synthetic data.
                </p>
                <p class="thanks">
                    Special thanks to John U. Balis and Adam Labiosa for helping with the physical robotics experiments.
                </p>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/da2.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2310.17786">Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Josiah P. Hanna<br/>
                <span class="italic">International Conference on Learning Representations (<a href="https://iclr.cc">ICLR</a></span>), 2024<br/>
                <a class="btn" href="https://arxiv.org/abs/2310.17786">arXiv</a> / <a class="btn" href="bibtex/corrado2024understandingDA.txt">bibtex</a>
<!--                <a class="btn" href="https://iclr.cc/virtual/2024/poster/17643">video </a> / <a class="btn" href="files/iclr_2024_understanding_da_poster.pdf">poster</a> / <a class="btn" href="files/iclr_2024_understanding_da.pdf">slides</a> / -->
                <p>
                    <span class="bold">TLDR:</span> Several prior works introduce data augmentation techniques that improve the data efficiency of RL.
                    Rather than proposing yet another method, we ask: <em>when</em> and <em>why</em> does data augmentation help?
                    Our work offers practical insights to help RL practitioners apply data augmentation more effectively.
                </p>
            </div>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/power2.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://ieeexplore.ieee.org/document/10333881">Deep Reinforcement Learning for Distribution Power System Cyber-Resilience via Distributed Energy Resource Control</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Michael Livesay, Tyson Bailey, & Drew Levin.<br/>
                <span class="italic">IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids, (<a href="https://sgc2023.ieee-smartgridcomm.org">IEEE SmartGridComm</a>)</span>, 2023<br/>
                <a class="btn" href="https://ieeexplore.ieee.org/document/10333881">paper</a> / <a class="btn" href="bibtex/corrado2023power.txt">bibtex</a>
                <br> <br/>
<!--                <details>-->
<!--                    <summary>Abstract</summary>-->
<!--                    <p>-->
<!--                        Interoperable, internet-connected distributed energy resource (DER) devices in power systems create a new attack vector for malicious cyber-actors. Modern control systems primarily focus on transmission and sub-transmission operations and rarely incorporate DER or other distribution-connected equipment. While recent advances have expanded grid operator visibility and control functionality to the distribution level, these control systems struggle to scale with thousands of networked devices. Thus, to defend against potential attacks on DER devices, we must develop new real-time control algorithms to ensure grid stability. In this work, we present a new approach to power distribution control based on deep reinforcement learning (RL) algorithms which can learn optimal control policies through experience. We evaluate four RL algorithms in novel voltage stabilization tasks based on the IEEE 13-bus and EPRI Ckt5 power distribution models. We demonstrate that RL can successfully perform voltage regulation and outperform a greedy algorithm. Our results coupled with the established capability of RL to scale in high-dimensional settings opens a path towards real-time cyber defense of power systems with RL agents.-->
<!--                    </p>-->
<!--                </details>-->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/salas.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://proceedings.mlr.press/v199/corrado22a/corrado22a.pdf">Simulation-Acquired Latent Action Spaces for Dynamics Generalization</a><br/>
                <span class="bold">Nicholas E. Corrado</span>, Yuxiao Qu, & Josiah P. Hanna<br/>
                <span class="italic">Conference on Lifelong Learning Agents (<a href="https://lifelong-ml.cc">CoLLAs</a>)</span>, 2022<br/>
                <a class="btn" href="https://proceedings.mlr.press/v199/corrado22a">paper</a> / <a class="btn" href="bibtex/corrado2022salas.txt">bibtex</a>
                <br> <br/>
<!--                <details>-->
<!--                    <summary>-->
<!--                      Abstract-->
<!--                    </summary>-->
<!--                    <p>-->
<!--                        Deep reinforcement learning has shown incredible promise at training high-performing agents to solve high-dimensional continuous control tasks in a particular training environment. However, to be useful in real-world settings, long-lived agents must perform well across a range of environmental conditions. Naively applying deep RL to a task where environment conditions may vary from episode to episode can be data inefficient. To address this inefficiency, we introduce a method that discovers structure in an agent’s high-dimensional continuous action space to speed up learning across a range of environmental conditions. Whereas prior work on finding so-called latent action spaces requires expert demonstrations or on-task experience, we instead propose to discover the latent, lower-dimensional action space in a simulated source environment and then transfer the learned action space for training in the target environment. We evaluate our novel method on randomized variants of simulated MuJoCo environments and find that, when there is a lower-dimensional action-space to exploit, our method significantly increases data efficiency. For instance, in the Ant environment, our method reduces the 8-dimensional action-space to a 3-dimensional action-space and doubles the average return achieved after a training budget of 2 million timesteps.-->
<!--                    </p>-->
<!--                </details>-->
            </div>
        </div>
<!--        <div>-->
<!--            <h2 class="noselect">Github</h2>-->
<!--            <div class="row">-->
<!--              <div class="column">-->
<!--                <img src="https://github-readme-stats.vercel.app/api?username=NicholasCorrado&hide_border=false&show_icons=true&theme=default&include_all_commits=true" width="100%"/>-->
<!--              </div>-->
<!--              <div class="column">-->
<!--                <img src="https://github-trophies.vercel.app/?username=NicholasCorrado&theme=bluefy&no-frame=false&no-bg=true&margin-w=4&row2&column=4&rank=SECRET,SSS,SS,S,AAA,AA,A,B,C" width="100%"/>-->
<!--              </div>-->
<!--            </div>-->
<!--        </div>-->
        <div>
            <h2 class="noselect">Personal</h2>
            Outside of research, <a href="https://www.youtube.com/@nicholas.e.corrado/videos">I am a jazz guitarist</a> &mdash; in fact, I almost pursued music professionally after high school.
            I am heavily influenced by gypsy jazz (or jazz manouche) but also draw inspiration from bossa, Dixieland, and modal jazz styles.
            When I was 13, I had the great fortune of meeting and playing with <a href="https://pittsburghsymphony.org/pso_home/biographies/guest-artists/joe-negri">Joe Negri</a> (aka <a href="http://www.neighborhoodarchive.com/mrn/characters/handyman_negri/index.html">"Handyman Negri" from Mister Rogers' Neighborhood</a>).
            He is a jazz legend in my hometown of Pittsburgh and arguably one of the best jazz guitarists in the US.
        </div>
    </div>
</div>

<div class="footer noselect">
    <div class="footer-content">
        This website layout was inspired by <a href="https://www.nicklashansen.com">Nicklas Hansen's</a> website.
    </div>
</div>